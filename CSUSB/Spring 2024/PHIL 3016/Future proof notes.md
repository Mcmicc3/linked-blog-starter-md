Rule #1. Be surprising, social, and scarce
![[Pasted image 20240510144920.png]]
![[Pasted image 20240510145018.png]]
![[Pasted image 20240510145033.png]]
![[Pasted image 20240510145119.png]]

![[Pasted image 20240510145421.png]]


Rule #2. Resist machine drift
![[Pasted image 20240510145851.png]]
![[Pasted image 20240510145919.png]]
rule #3 Demote your devices
![[Pasted image 20240510151405.png]]
![[Pasted image 20240510151451.png]]

Rule #4 Leave Handprints
![[Pasted image 20240510151528.png]]
![[Pasted image 20240510151546.png]]


## Rule #2

The idea of "machine drift". There are things in your life that are very easy to predict. What you like to eat, watch, dress. Now we have algorithms on social media and youtube that can predict what you like to watch or read, and see you can mindlessly go down a rabbit hole watching things, or accepting all the suggested text given to us by AI (responses to emails like OKAY, or WILL DO). AAccpeint whatever the algorithm gives you, puts you in what the author calls machine drift

Machine drift can happen outside of technology too. When you become very politcally polarizing, or questioning if you actually liked the shoes you bought online, or if you only trusted the algorithms recommendation. 

The story of tapestry, an algorithm that sorts emails by their importance and relevancy, was the start of the **recommendation engine**.

Recommender systems of the past  were designed to save us time, but many of today's recommenders are designed to take time from us. 

80 percent of movies viewed on Netflix, came from recommendations. They claim it saved them a billion dollars a year. The psychological power of recommendations was made clear in a 2018 study led by Gediminas Adomavicius, a professor at the University of Minnesota. 

Choice archetecture could be good for apps like Yelp that show you suggestions close to you, and not waste your time with bad suggestions. However, companies can be biased to their own products, like Amazon, or Netflix (can steer you to their original shows). "The pwoer to change users preferences at scale has moade some technologist uncomforatable. Rachel Schutt, a data scientists, said as much in a 2012 interview with the times. "**Models do not just predict, but they can make things happen**", A former product manager from Facebook said that recommendation algorithms amounted to an attempt to *reprogram humans*". 

People walk in, and disregard their own experiences and prior beliefs once they're recommended something by the algorithm. 
In recent years, techn companies have figured out that mind chaniging algorithms area a lucrative prospect. **Targeted advertising** - the business that has made Google and Facebook two of ht em ost valuable compaines in the world -- combines both "**Read our minds** " and "**Change our minds**" technology by analyzing data to guess users' preferences, then allowing advertisers to pay to attempt to change their minds. 

Today's recommender algorithms are so powerful and so deeply embedded in our systems, that they often function more like *decider algorithms*. By ranking certain informationm ore highly, or prioritiuzung the choices ina   certain way, they can create the illusion of free will, while actually steering users down a path toward their preferred outcome.  What makes **machine drift so dangerous** is that at a time whenwe most need our human faculties to help guide us, these algorithms are actively eroding th eparts of ourselves that makle us most human: *our ability to change course, to pursue difficult goals, to make unpopular choices that cut agains the grain.* They're discouraging us from building the jidn of personal autoonomy that will protect us in the age of AI and automation, by allowing us to think andact for ourselves. And they're doing it under the guise of helping us.

If there is less friction in doing something, we are more likely to do it. Whatever tech service is being offered, their goal is to make it frictionless so it is seemessly easy to sign up for the service. Often "eliminating friction" in a tech product simply means transgerring the burden to a low-paid worker. These models are teaching us to always pick the easy option, and not do things like trying new experiences, or overcoming tough obstacles. 

**Resisting machine drift**. Take inventory of your own preferences. Think of all your daily choices, and determine which of those choices are truly yours, and which are fundamentally shaped by a machines instructions or suggestions. 

Another way to resist is to implement what he calls *Human hour*. Do something that doesn't involve looking at a screen for an hour. Do things like read a newspaper, steam some milk for your coffee, take the scenic route home, these are all *minor inconviences*. 


## Middle of rule 3

Funny experiment of participants shocking themselves rather than having to deal with their own boredom. 

The intention of the detox program was to to rediscover what was fascinating and energizing about the offline world. *Think of the bigger picutre of what you're getting by not being on Twitter all the time.* 

**Rubber band on the phone experiment** 

Keep the phone outside the bedroom at night before sleep. Delete distracting apps, 

Three resutls from demoting their devices after catherines mobile detox prigram
1. More appreciative of all the other devices in his life. He got a sense of wonder from this technology again
2. More productive, in having more ideas, inspired to do projects like this book. More emotionally perspective to others
3. Other people started using their devices less. 

