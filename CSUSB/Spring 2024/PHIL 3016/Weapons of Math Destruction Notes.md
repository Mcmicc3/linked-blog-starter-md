Impact Evaluation 

Pg 6, a statistical model needs feedback to verify it's accuracy. Examples like the school district, is a situation where he model defines their own reality and use it to justify their results. It never learns if it is right (pg 7). "WMD feedback loop". WMDs target the poor. For example, people with bad credit are less likely to find work. If the details are hidden, it's harder to question the score, or protest against it. These models are often complex. 

How do you justify evaluating people by a measure for which you are unable to provide explanation? " Pg 8. 
You cannot appeal to a WMD. Story of professor who had an incoming class to students 5x the average reading performance of their peers, found they could barely read sentences. Great likelyjood of cheating. During the recession, there was an incentive to cheat so teachers could keep their jobs, as well as get an 8,000 bonus. This would make the next year's professor seem like she's making the students do even worse. (Pg 8)

Programming is expensive. Once the algorithm is done, somebody probably decides to go the cheap option and say it works fine. 

A poor school with a good teacher, was transfered to a good school where they didn't fire based off of grades. Suggestive evidence is not good enough against a WMD. The case must be ironclad, where's the WMD only suggests that something MIGHT, be correct. Author started a blog against sloppy statistics and toxic feedback loops. (Pg 9) 

WMDs routinely lack data for the behaviors they're most interested in. So they substitute stand-in data, or proxies. They make correlations between zip codes or language patterns for potential pay back a loan, or handle a job. These correlations are discriminatory, and some are illegal  (pg 14)

Cooking for your family example, results, what they like, your own energy, your ingredients, their satisfaction, you adjust your model to their satisfaction. That's called a "dynamic model".  (Pg 15)

Someone asks for advice on your exact cooking model, you get ambitious and decided to summon everything you know into a computer program. You gradually get all the minor details, you then go from an informal internal model in your head, to a formal external model. An autonomous you for when you're not around. There will be mistakes. Model, by nature are just simplifications. No model can capture all of life's real world complexities. You can neglect information, like suggested food on birthdays, or who like raw carrots and who likes them cooked. We make models based on what's important to include. A clueless machine with enormous blind spots. Blind spots can be good, Google maps doesn't show detailed buildings because they're irrelevant to the task. A models blind spots reflect the judgments and priorities of it's creators. In the school example, it's focus is on test scores, but the blind spots are on teachers engagement with students, work on specific skills, classroom management, or helping students with personal and family problems. It sacrifices accuracy for efficiency. (Pg 16)

We impose our own ideology in models. Example, removing Poptarts from the cooking model. What if north korea made the model? or her kids? the priorities and what success would look like, would be different in comparisson to who made the model. 
It will also grow stale if it's not being updated. Prices change, things go out of stock, preferences change. A model built for a six year old, won't work on a teenager. 
Simple models do exist, fire alarms only rely on one vairable, the presence of smoke, (pg 17)

Same model idea can be used on the ideology of racism. (pg 18)

Talks of computerized risk models that courts use to determine whether a criminal is likely to re offend, and using that model to decide their prison sentence. The unfairness this is for people of different backgrounds and up bringings. How this made blacks three times more likely to get the death sentence and Hispanics four times more likely in Texas.  (19)

Blacks and hispanics are more likely to randomly be stopped and frisked. These type of questions come up when asked in the court, and that information feeds the algorithm, despite 90% of them being innocent. They also get asked whether they have any family members with criminal records, they responses are likely to be different depending on where you grew up. It's illegal to ask about race, but with the wealth of detail each prisoner provides, it doesn't matter. 
**LSI-R quiestionnare**. In some states, it only effects people with high risk scores, but in other states it is used to guide their sentencing. 
The questionnaire asks about circumstances from birth, upbringing (including friends and family), neighborhood, and friends. These type of questions should not be relevant in deciding the sentencing of a criminal case. What's ironic, is that these type of questions would get an objection if you try to bring up those same questions in court. 
**Recidivism risk.**  (20)

If someone from a poor nieghborhood goes to jail, they'll get out and return to the same place, now with a criminal record, which makes it harder to find a job, and if they get arrested again, it would be considered a success to the recidivism model, which itself contributes to a toxic cycle and helps to sustain it. (21)

Prison officers keep quiet of the LSI-Rs purpose in fear that if innmates knew that it would be used against them, they would purposely give false information to lesson their sentence. 
Transparncey matters, but for some companies, the model is "intellectual property" and it must be defended. 
WMDs for Google, Facebook, and Amazon are worth billions of dollars. They are black boxes by design which makes it harder to ask if they're built against my interest? Are they unfair? Does it damage or destroy lives? (22)

WMDs are trusted, to the point that they're almost the law. More examples of it being used against you include banks deciding if you're a high risk borrower, credit models, and if you're on the wrong end of the scale, it will treat you as a deadbeat, regardless if you're misunderstood. Few people benefit, but the point is not that some people are lucky, it's that so many people suffer.  (23 - 24)

Suggesting that the rankings of U.S News, in suggesting which the best colleges were, is treated like destiny, because if your school is low, top students, top teachers, and alumni donors would all refuse to participate. The model can't measure learning, happiness, confidence, friendships, or other aspects of a students four year expereince. 
Before, a school success would be determined based on what students would become, and is spread through word of mouth. 27

Some tried getting there by cheating. 28

Texas Christian University in Fort Worth Texas was dropping from 97th place to 105, 108, and now 113. The chancellor was in the hot seat, and suggested that TCU was advancing in every way. "Retention rate is improving, fundraising, all the things they go on are improving"

If other schools advanced faster, they would fall behind. Reputation is 25%. 

Doing well in sports is the most effectice promotion to a school for some applicants. Rasising money to fund the school and improve programs to draw more students, which in turn improves the U.S News algorithm.  30

With algorithms like U.S News, it causes everyone to shoot for the same goal, which creaetes a rat race. This removed the concept of a "safety school". You could apply to where you want to be, and also apply for other schools which you felt like it was a safe bet to get in. Those schools are extinct now. 31

This algortithm, this type of spending the school makes, has contributed to the increase of student tuition. We get all these fancy things, but that's because we're the ones paying for it through student loans. 33

Students in china were caught cheating, they protested for the right to cheat, because they believed other schools were cheating too, which wouldn't give them a chance at a prosperous career. 35

t’s easy to raise graduation rates, for example, by lowering standards. Many  
students struggle with math and science prerequisites and foreign languages. Water  
down those requirements, and more students will graduate. But if one goal of our  
educational system is to produce more scientists and technologists for a global  
economy, how smart is that?
**We're trading the intended outcomes of getting an education, for this model**. 